{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 478: Machine Learning</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Convolutional Neural Networks CNN</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b>Convolutional Neural Networks</b>\n",
    "    - Computer Vision Applications\n",
    "    - Convolution Operation\n",
    "    - Pooling Operation\n",
    "    - Padding and Strides\n",
    "    - CNN Architectures\n",
    "        - LeNet-5\n",
    "        - AlexNet\n",
    "            - Data Augmentation\n",
    "        - GoogleNet\n",
    "            - Inception Module\n",
    "        - VGGNet\n",
    "        - ResNet\n",
    "            - Residual Learning\n",
    "        - Xception\n",
    "        - SENet\n",
    "- <b>Further Applications</b>\n",
    "    - Object Detection (IoU & Non-Max Suppression)\n",
    "    - Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Computer Vision</center></h1>\n",
    "\n",
    "- Computer Vision is one of the areas that has been advancing rapidly thanks to Deep Learning and specifically **CNN**s.\n",
    "\n",
    "\n",
    "- CV in self-driving cars includes different ML tasks such as: image classification, object detection, image segmentation, etc. \n",
    "\n",
    "<center><img src=\"img/computer-vision.gif\" align=\"center\"/></center>\n",
    "\n",
    "<center><img src=\"img/cv-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[4]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Image Classification vs Object Detection</center></h1>\n",
    "\n",
    "- **Image classification** takes an image as an input and outputs the classification label of that image with some metric (probability score, confidence, etc). \n",
    "\n",
    "\n",
    "- **Object detection** is the process of finding instances of objects in images. The task of classifying and localizing multiple objects in an image is called **object detection**.\n",
    "\n",
    "<center><img src=\"img/object-localization.png\" align=\"center\"/></center>\n",
    "<font size=1>Image from: https://www.kaggle.com/getting-started/169984</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Object Detection Examples</center></h1>\n",
    "\n",
    "[Tensorflow](https://www.tensorflow.org/lite/models/object_detection/overview)\n",
    "\n",
    "[YOLO](https://github.com/pjreddie/darknet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Neural Style Transfer</center></h1>\n",
    "\n",
    "[Tensorflow Tutorial](https://www.tensorflow.org/tutorials/generative/style_transfer)\n",
    "\n",
    "<center><img src=\"img/neural-style-transfer.jpeg\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Image and Video Colorization</center></h1>\n",
    "\n",
    "[Image Colorization API from DeepAI](https://deepai.org/machine-learning-model/colorizer)\n",
    "\n",
    "\n",
    "[DeOldify - A library in GitHub](https://github.com/jantic/DeOldify)\n",
    "\n",
    "\n",
    "[Coloring Movie Psycho (1960)](https://www.youtube.com/watch?v=l3UXXid04Ys&feature=emb_logo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Motivation - Large Images & Too Many Parameters</center></h1>\n",
    "\n",
    "<center><img src=\"img/large-images.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Motivation - Convolution Operation</center></h1>\n",
    "\n",
    "- Convolutional networks have been tremendously successful in CV practical applications. The name **convolutional neural network** indicates that the network employs a mathematical operation called **convolution**.\n",
    "\n",
    "\n",
    "- **Convolution** is a specialized kind of linear operation.\n",
    "\n",
    "\n",
    "- Convolutional networks are simply neural networks that use **convolution** in place of general matrix multiplication in at least one of their layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Motivation - Convolution Operation</center></h1>\n",
    "\n",
    "- **Convolution** leverages three important ideas that can help improve a machine learning system:\n",
    "    - Sparse interactions\n",
    "\n",
    "    - Parameter sharing\n",
    "\n",
    "    - Equivariant representations\n",
    "\n",
    "\n",
    "- Moreover, **convolution** provides a means for working with inputs of variable size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Edge Detection Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/edge-detection.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Operation - Vertical Edge Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/conv-op-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Operation - Vertical Edge Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/conv-op-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Operation - Vertical Edge Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/conv-op-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Operation - Vertical Edge Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/conv-op-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Operation Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/conv-op-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Operation Animation</center></h1>\n",
    "\n",
    "<center><img src=\"img/convolution-operation-1.gif\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1> Image from Ref[13]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Operation Example-1</center></h1>\n",
    "\n",
    "<center><img src=\"img/convolution-operation-2.gif\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1> Image from Ref[13]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Operation Example-2</center></h1>\n",
    "\n",
    "<center><img src=\"img/convolution-operation-3.gif\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1> Image from Ref[15]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Inspiration from Visual Cortex</center></h1>\n",
    "\n",
    "<center><img src=\"img/visual-cortex.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Layers</center></h1>\n",
    "\n",
    "<center><img src=\"img/cnn-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Filters and Feature Maps</center></h1>\n",
    "\n",
    "<center><img src=\"img/feature-map-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Padding</center></h1>\n",
    "\n",
    "<center><img src=\"img/padding.gif\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1> Image from Ref[13]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Zero Padding</center></h1>\n",
    "\n",
    "<center><img src=\"img/padding-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>\"same\" Padding</center></h1>\n",
    "\n",
    "- Pad so that output size is the same as input size:\n",
    "\n",
    "    - $n+2p-f+1 = n \\implies p = \\frac{f-1}{2}$\n",
    "    \n",
    "    \n",
    "- Note: Here stride is assumed to be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Padding \"same\" vs \"valid\"</center></h1>\n",
    "\n",
    "- If set to \"same\" , the convolutional layer uses zero padding if necessary.\n",
    "\n",
    "- Then zeros are added as evenly as possible around the inputs, as needed. When strides=1 , the layer’s outputs will have the same spatial dimensions (width and height) as its inputs, hence the name \"same\".\n",
    "\n",
    "<center><img src=\"img/padding-same-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Strided Convolution</center></h1>\n",
    "\n",
    "<center><img src=\"img/strided.gif\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1> Image from Ref[13]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Strided Convolution Example - 1</center></h1>\n",
    "\n",
    "<center><img src=\"img/strided-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Strided Convolution Example - 2</center></h1>\n",
    "\n",
    "<center><img src=\"img/strided-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Strided Convolution Example - 3</center></h1>\n",
    "\n",
    "<center><img src=\"img/strided-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Strided Convolution Example - 4</center></h1>\n",
    "\n",
    "<center><img src=\"img/strided-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Strided Convolution - Computing Output Dimension</center></h1>\n",
    "\n",
    "$$\\frac{n + 2p - f}{s} + 1$$\n",
    "\n",
    "- where:\n",
    "    - n: input size\n",
    "    - f: filter size\n",
    "    - p: padding\n",
    "    - s: stride\n",
    "      \n",
    "\n",
    "- If the result is not integer, round it down, i.e. use floor() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution Summary</center></h1>\n",
    "\n",
    "<center><img src=\"img/conv-summary.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Strided Convolution Effect - Dimensionality Reduction</center></h1>\n",
    "\n",
    "<center><img src=\"img/striding-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convolution over Volume</center></h1>\n",
    "\n",
    "<center><img src=\"img/conv-3D.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>Convolution Over RGB Channels</center></h1>\n",
    "\n",
    "<center><img src=\"img/rgb.gif\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1> Image from Ref[13]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Equation 14-1: Computing the output of a neuron in a convolutional layer**\n",
    "\n",
    "$\n",
    "z_{i,j,k} = b_k + \\sum\\limits_{u = 0}^{f_h - 1} \\, \\, \\sum\\limits_{v = 0}^{f_w - 1} \\, \\, \\sum\\limits_{k' = 0}^{f_{n'} - 1} \\, \\, x_{i', j', k'} \\times w_{u, v, k', k}\n",
    "\\quad \\text{with }\n",
    "\\begin{cases}\n",
    "i' = i \\times s_h + u \\\\\n",
    "j' = j \\times s_w + v\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stacking Multiple Feature Maps</center></h1>\n",
    "\n",
    "- A convolutional layer has multiple filters (you decide how many) and outputs one feature map per filter, so it is more accurately represented in 3D (see Figure 14-6).\n",
    "\n",
    "- It has one neuron per pixel in each feature map, and all neurons within a given feature map share the same parameters (i.e., the same weights and bias term). Neurons in different feature maps use different parameters. A neuron’s receptive field extends across all the previous layers’ feature maps.\n",
    "\n",
    "- In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs.\n",
    "\n",
    "<center><img src=\"img/conv-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Shared Parameters in CNN</center></h1>\n",
    "\n",
    "\n",
    "- The fact that all neurons in a feature map **share the same parameters** dramatically reduces the number of parameters in the model.\n",
    "\n",
    "\n",
    "- Once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location. \n",
    "\n",
    "\n",
    "- In contrast, once a regular DNN has learned to recognize a pattern in one location, it can recognize it only in that particular location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Sparse Connectivity</center></h1>\n",
    "\n",
    "- Traditional neural network layers use matrix multiplication by a matrix of parameters with a separate parameter describing the interaction between each input unit and each output unit. This means that every output unit interacts with every input unit. \n",
    "\n",
    "\n",
    "- Convolutional networks, however, typically have sparse interactions---also referred to as **sparse connectivity** or **sparse weights**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>Effect of Sparse Connectivity - Viewed from Below</center></h1>\n",
    "\n",
    "<center><img src=\"img/sparse-connectivity-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>Effect of Sparse Connectivity - Viewed from Above</center></h1>\n",
    "\n",
    "<center><img src=\"img/sparse-connectivity-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Pooling</center></h1>\n",
    "\n",
    "- The goal of **pooling** is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting).\n",
    "\n",
    "- Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the inputs using an aggregation function such as the max or mean.\n",
    "\n",
    "- Figure 14-8 shows a max pooling layer, which is the most common type of pooling layer. In this example, we use a 2 × 2 pooling kernel, with a stride of 2 and no padding. Only the max input value in each receptive field makes it to the next layer, while the other inputs are dropped.\n",
    "\n",
    "- For example, in the lower-left receptive field in Figure 14-8, the input values are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the stride of 2, the output image has half the height and half the width of the input image (rounded down since we use no padding).\n",
    "\n",
    "<center><img src=\"img/pooling-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Max Pooling</center></h1>\n",
    "\n",
    "- Other kernels we’ve discussed so far had trainable weights, but pooling kernels do not: they are just stateless sliding windows.\n",
    "\n",
    "<center><img src=\"img/max-pooling.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1> Image from Ref[14]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Pooling Effect - Translation Invariance</center></h1>\n",
    "\n",
    "- Other than reducing computations, memory usage, and the number of parameters, a max pooling layer also introduces some level of invariance to small translations, as shown in Figure 14-9. Here we assume that the bright pixels have a lower value than dark pixels, and we consider three images (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but shifted by one and two pixels to the right.\n",
    "\n",
    "- As you can see, the outputs of the max pooling layer for images A and B are identical. This is what **translation invariance** means.\n",
    "\n",
    "- For image C, the output is different: it is shifted one pixel to the right (but there is still 75% invariance). By inserting a max pooling layer every few layers in a CNN, it is possible to get some level of translation invariance at a larger scale.\n",
    "\n",
    "- Moreover, max pooling offers a small amount of rotational invariance and a slight scale invariance. Such invariance (even if it is limited) can be useful in cases where the prediction should not depend on these details, such as in classification tasks.\n",
    "\n",
    "<center><img src=\"img/pooling-invariance-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Depthwise Pooling</center></h1>\n",
    "\n",
    "- Note that max pooling and average pooling can be performed along the depth dimension rather than the spatial dimensions, although this is not as common. This can allow the CNN to learn to be invariant to various features.\n",
    "\n",
    "- For example, it could learn multiple filters, each detecting a different rotation of the same pattern (such as handwritten digits; see Figure 14-10), and the depthwise max pooling layer would ensure that the output is the same regardless of the rotation.\n",
    "\n",
    "- The CNN could similarly learn to be invariant to anything else: thickness, brightness, skew, color, and so on.\n",
    "\n",
    "<center><img src=\"img/pooling-invariance-2-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>CNN Architecture</center></h1>\n",
    "\n",
    "- Typical CNN architectures stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on.\n",
    "\n",
    "<center><img src=\"img/cnn-architecture-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>CNN Architecture</center></h1>\n",
    "\n",
    "- The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps), thanks to the convolutional layers.\n",
    "\n",
    "- At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).\n",
    "\n",
    "<center><img src=\"img/cnn-mathworks.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[15]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>CNN in Tensorflow</center></h1>\n",
    "\n",
    "- In TensorFlow, each input image is typically represented as a 3D tensor of shape [height, width, channels].\n",
    "\n",
    "\n",
    "- A mini-batch is represented as a 4D tensor of shape [mini-batch size, height, width, channels].\n",
    "\n",
    "\n",
    "- The weights of a convolutional layer are represented as a 4D tensor of shape [$f_h , f_w , f_n′ , f_n$].\n",
    "\n",
    "\n",
    "- The bias terms of a convolutional layer are simply represented as a 1D tensor of shape [$f_n$]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# CNN Example: The following code works on a 10-class image classification with input size: 32x32x3\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                        input_shape=(32, 32, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')) # number of filters usually grows\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')) \n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu')) # FC Layer\n",
    "model.add(layers.Dense(10, activation='softmax')) # Output layer for 10-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_42 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 5, 5, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 159,434\n",
      "Trainable params: 159,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>CNN Architectures and ImageNet</center></h1>\n",
    "\n",
    "- Over the years, variants of this fundamental architecture have been developed, leading to amazing advances in the field.\n",
    "\n",
    "- A good measure of this progress is the error rate in competitions such as the ILSVRC **ImageNet** challenge.\n",
    "\n",
    "- In this competition the top-five error rate for image classification fell from over 26% to less than 2.3% in just six years.\n",
    "\n",
    "- The top-five error rate is the number of test images for which the system’s top five predictions did not include the correct answer.\n",
    "\n",
    "- The images are large (256 pixels high) and there are 1,000 classes, some of which are really subtle (try distinguishing 120 dog breeds).\n",
    "\n",
    "- Looking at the evolution of the winning entries is a good way to understand how CNNs work.\n",
    "\n",
    "- Keras has implementations of multiple CNN architectures: https://keras.io/api/applications/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Using CNN Architectures</center></h1>\n",
    "\n",
    "\n",
    "- In general, you won’t have to implement standard models like GoogLeNet or ResNet manually, since pretrained networks are readily available with a single line of code in the [`keras.applications`](https://keras.io/api/applications/) package.\n",
    "\n",
    "\n",
    "- For example, you can load the ResNet-50 model, pretrained on ImageNet, with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,636,712\n",
      "Trainable params: 25,583,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''- The global average pooling layer computes the mean activation for each feature map:\n",
    "for example, if its input contains 256 feature maps, \n",
    "it will output 256 numbers representing the overall level of response for each filter.'''\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Transfer Learning with CNNs</center></h1>\n",
    "\n",
    "- If you want to build an image classifier but you do not have enough training data, then it is often a good idea to reuse the lower layers of a pretrained model, as we discussed Transfer Learning in DNNs.\n",
    "\n",
    "\n",
    "- For example, you can train a model to classify pictures of flowers, reusing a pretrained **Xception** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''- Load an Xception model, pretrained on ImageNet.\n",
    "- We exclude the top of the network by setting `include_top=False` : \n",
    "    this excludes the global average pooling layer and the dense output layer.\n",
    "- We then add our own global average pooling layer, \n",
    "    based on the output of the base model, followed by a dense output layer with one unit per class,\n",
    "    using the softmax activation function. Finally, we create the Keras Model: '''\n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>LeNet-5 Architecture</center></h1>\n",
    "\n",
    "- The LeNet-5 architecture is perhaps the most widely known CNN architecture. As mentioned earlier, it was created by Yann LeCun in 1998 and has been widely used for handwritten digit recognition (MNIST). It is composed of the layers shown in Table 14-1.\n",
    "\n",
    "<center><img src=\"img/lenet-5-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>AlexNet</center></h1>\n",
    "\n",
    "- The AlexNet CNN architecture won the 2012 ImageNet ILSVRC challenge by a large margin: it achieved a top-five error rate of 17%, while the second best achieved only 26%!\n",
    "\n",
    "\n",
    "- AlexNet was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton.\n",
    "\n",
    "\n",
    "- AlexNet is similar to LeNet-5, only much larger and deeper, and it was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer.\n",
    "\n",
    "\n",
    "- To reduce overfitting, the authors used two regularization techniques:\n",
    "    - First, they applied dropout with a 50% dropout rate during training to the outputs of layers F8 and F9.\n",
    "    - Second, they performed **data augmentation** by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>AlexNet Architecture</center></h1>\n",
    "\n",
    "<center><img src=\"img/alexnet-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Data Augmentation</center></h1>\n",
    "\n",
    "- Data augmentation artificially increases the size of the training set by generating many realistic variants of each training instance.\n",
    "\n",
    "\n",
    "- This reduces overfitting, making this a regularization technique.\n",
    "\n",
    "\n",
    "- The generated instances should be as realistic as possible: ideally, given an image from the augmented training set, a human should not be able to tell whether it was augmented or not. Simply adding white noise will not help; the modifications should be learnable (white noise is not).\n",
    "\n",
    "\n",
    "- For example, you can slightly shift, rotate, and resize every picture in the training set by various amounts and add the resulting pictures to the training set.\n",
    "\n",
    "\n",
    "- This forces the model to be more tolerant to variations in the position, orientation, and size of the objects in the pictures.\n",
    "\n",
    "\n",
    "- For a model that’s more tolerant of different lighting conditions, you can similarly generate many images with various contrasts.\n",
    "\n",
    "\n",
    "- In general, you can also flip the pictures horizontally (except for text, and other asymmetrical objects). By combining these transformations, you can greatly increase the size of your training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Data Augmentation</center></h1>\n",
    "\n",
    "<center><img src=\"img/data-aug-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Image Augmentation</center></h1>\n",
    "\n",
    "https://github.com/aleju/imgaug\n",
    "\n",
    "<center><img src=\"img/imgaug.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[6]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Image Augmentation Using Tensorflow ImageDataGenerator()</center></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create data generator\n",
    "data_generator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "# Prepare iterator\n",
    "iterate_train = data_generator.flow(X_train, y_train, batch_size=64)\n",
    "steps = int(train_images.shape[0] / 64)\n",
    "\n",
    "# Train the model using the iterator\n",
    "# Note: In the previous tf versions, fit_generator() was used to train generators instead of fit() \n",
    "# fit() now supports generators, so there is no longer any need to use fit_generator()\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator\n",
    "\n",
    "history = model.fit(iterate_train, steps_per_epoch=steps, validation_data=(X_test, y_test),\n",
    "                              epochs=EPOCHS, callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GoogleNet - Inception Module</center></h1>\n",
    "\n",
    "- GoogLeNet uses subnetworks called **inception modules** which allow to use parameters much more efficiently than previous architectures: GoogLeNet actually has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).\n",
    "\n",
    "- Figure 14-13 shows the architecture of an inception module. The notation “3 × 3 + 1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and \"same\" padding. The input signal is first copied and fed to four different layers. All convolutional layers use the ReLU activation function.\n",
    "\n",
    "- Note that the second set of convolutional layers uses different kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different scales.\n",
    "\n",
    "- Also note that every single layer uses a stride of 1 and \"same\" padding (even the max pooling layer), so their outputs all have the same height and width as their inputs. This makes it possible to concatenate all the outputs along the depth dimension in the final depth concatenation layer (i.e., stack the feature maps from all four top convolutional layers).\n",
    "\n",
    "- This concatenation layer can be implemented in TensorFlow using the `tf.concat()` operation, with `axis=3` (the axis is the depth).\n",
    "\n",
    "<center><img src=\"img/inception-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Inception Module</center></h1>\n",
    "\n",
    "- You may wonder why inception modules have convolutional layers with 1 × 1 kernels. Can these layers capture any features because they look at only one pixel at a time?\n",
    "\n",
    "- In fact, the layers serve three purposes:\n",
    "    - Although they cannot capture spatial patterns, they can capture patterns along the depth dimension.\n",
    "    - They are configured to output fewer feature maps than their inputs, so they serve as bottleneck layers, meaning they reduce dimensionality. This cuts the computational cost and the number of parameters, speeding up training and improving generalization.\n",
    "    - Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like a single powerful convolutional layer, capable of capturing more complex patterns.\n",
    "\n",
    "- Indeed, instead of sweeping a simple linear classifier across the image (as a single convolutional layer does), this pair of convolutional layers sweeps a two-layer neural network across the image.\n",
    "\n",
    "- In short, you can think of the whole inception module as a convolutional layer on steroids, able to output feature maps that capture complex patterns at various scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>GoogleNet Architecture</center></h1>\n",
    "\n",
    "- Now let’s look at the architecture of the GoogLeNet CNN (see Figure 14-14). The number of feature maps output by each convolutional layer and each pooling layer is shown before the kernel size.\n",
    "\n",
    "- The architecture is so deep that it has to be represented in three columns, but GoogLeNet is actually one tall stack, including nine inception modules (the boxes with the spinning tops). The six numbers in the inception modules represent the number of feature maps output by each convolutional layer in the module (in the same order as in Figure 14-13).\n",
    "\n",
    "- Note that all the convolutional layers use the ReLU activation function.\n",
    "\n",
    "<center><img src=\"img/googlenet-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>VGGNet</center></h1>\n",
    "\n",
    "- The runner-up in the ILSVRC 2014 challenge was VGGNet, developed by Karen Simonyan and Andrew Zisserman from the Visual Geometry Group (VGG) research lab at Oxford University.\n",
    "\n",
    "\n",
    "- VGGNet has a very simple and classical architecture, with 2 or 3 convolutional layers and a pooling layer, then again 2 or 3 convolutional layers and a pooling layer, and so on (reaching a total of just 16 or 19 convolutional layers, depending on the VGG variant), plus a final dense network with 2 hidden layers and the output layer.\n",
    "\n",
    "\n",
    "- VGGNet uses only 3 × 3 filters, but many filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Residual Learning</center></h1>\n",
    "\n",
    "- As CNN models get deeper and deeper, training them gets more and more challenging, e.g. issues such as vanishing and exploding gradients may arise.\n",
    "\n",
    "\n",
    "- The key to being able to train such a deep network is to use skip connections (also called shortcut connections): the signal feeding into a layer is also added to the output of a layer located a bit higher up the stack.\n",
    "\n",
    "<center><img src=\"img/residual-learning-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Residual Network vs DNN</center></h1>\n",
    "\n",
    "- If you add many skip connections, the network can start making progress even if several layers have not started learning yet.\n",
    "\n",
    "- Thanks to skip connections, the signal can easily make its way across the whole network.\n",
    "\n",
    "<center><img src=\"img/dnn-resnet-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>ResNet Architecture</center></h1>\n",
    "\n",
    "<center><img src=\"img/resnet-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Skip Connection - Matching Dimensions</center></h1>\n",
    "\n",
    "- Note that the number of feature maps is doubled every few residual units, at the same time as their height and width are halved (using a convolutional layer with stride 2).\n",
    "\n",
    "- When this happens, the inputs cannot be added directly to the outputs of the residual unit because they don’t have the same shape.\n",
    "\n",
    "- To solve this problem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right number of output feature maps.\n",
    "\n",
    "<center><img src=\"img/skip-connection.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Xception</center></h1>\n",
    "\n",
    "- Another variant of the GoogLeNet architecture is worth noting: Xception (which stands for Extreme Inception) was proposed in 2016 by François Chollet (the author of Keras), and it significantly outperformed Inception-v3 on a huge vision task (350 million images and 17,000 classes).\n",
    "\n",
    "- Just like Inception-v4, it merges the ideas of GoogLeNet and ResNet, but it replaces the inception modules with a special type of layer called a depthwise separable convolution layer (or separable convolution layer for short).\n",
    "\n",
    "- While a regular convolutional layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer makes the strong assumption that spatial patterns and cross-channel patterns can be modeled separately. Thus, it is composed of two parts: the first part applies a single spatial filter for each input feature map, then the second part looks exclusively for cross-channel patterns—it is just a regular convolutional layer with 1 × 1 filters.\n",
    "\n",
    "<center><img src=\"img/depthwise.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SENet</center></h1>\n",
    "\n",
    "- The winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-Excitation Network (SENet).\n",
    "\n",
    "- This architecture extends existing architectures such as inception networks and ResNets, and boosts their performance.\n",
    "\n",
    "- This allowed SENet to win the competition with an astonishing 2.25% top-five error rate!\n",
    "\n",
    "- The extended versions of inception networks and ResNets are called SE-Inception and SE-ResNet, respectively.\n",
    "\n",
    "- The boost comes from the fact that a SENet adds a small neural network, called an **SE Block**, to every unit in the original architecture (i.e., every inception module or every residual unit).\n",
    "\n",
    "\n",
    "<center><img src=\"img/senet.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SE Block</center></h1>\n",
    "\n",
    "- An SE block analyzes the output of the unit it is attached to, focusing exclusively on the depth dimension (it does not look for any spatial pattern), and it learns which features are usually most active together.\n",
    "\n",
    "- It then uses this information to recalibrate the feature maps, as shown in Figure 14-21.\n",
    "\n",
    "- For example, an SE block may learn that mouths, noses, and eyes usually appear together in pictures: if you see a mouth and a nose, you should expect to see eyes as well. So if the block sees a strong activation in the mouth and nose feature maps, but only mild activation in the eye feature map, it will boost the eye feature map (more accurately, it will reduce irrelevant feature maps). If the eyes were somewhat confused with something else, this feature map recalibration will help resolve the ambiguity.\n",
    "\n",
    "<center><img src=\"img/se-block-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SE Block</center></h1>\n",
    "\n",
    "- An SE block is composed of just three layers: a global average pooling layer, a hidden dense layer using the ReLU activation function, and a dense output layer using the sigmoid activation function.\n",
    "\n",
    "<center><img src=\"img/se-block-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Object Localization</center></h1>\n",
    "\n",
    "\n",
    "- Localizing an object in a picture can be expressed as a regression task.\n",
    "\n",
    "\n",
    "- To predict a bounding box around the object, a common approach is to predict the horizontal and vertical coordinates of the object’s center, as well as its height and width. This means we have four numbers to predict.\n",
    "\n",
    "<center><img src=\"img/object-detection.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[7]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Intersection over Union (IoU)</center></h1>\n",
    "\n",
    "- The MSE often works fairly well as a cost function to train a model for regression, but it is not a great metric to evaluate how well the model can predict bounding boxes.\n",
    "\n",
    "\n",
    "- The most common metric for this is the Intersection over Union (IoU): the area of overlap between the predicted bounding box and the target bounding box, divided by the area of their union.\n",
    "\n",
    "\n",
    "- In `tf.keras`, it is implemented by the `tf.keras.metrics.MeanIoU` class.\n",
    "\n",
    "<center><img src=\"img/iou.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Object Detection</center></h1>\n",
    "\n",
    "- The task of classifying and localizing multiple objects in an image is called **object detection**.\n",
    "\n",
    "- Until a few years ago, a common approach was to take a CNN that was trained to classify and locate a single object, then slide it across the image, as shown below.\n",
    "\n",
    "- This technique is fairly straightforward, but as you can see it will detect the same object multiple times, at slightly different positions. Some post-processing will then be needed to get rid of all the unnecessary bounding boxes. A common approach for this is called **non-max suppression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Non-Max Suppression</center></h1>\n",
    "\n",
    "- Here is the process of **Non-Max Suppression**:\n",
    " 1. First, you need to add an extra objectness output to your CNN, to estimate the probability that a flower is indeed present in the image (alternatively, you could add a “no-flower” class, but this usually does not work as well). It must use the sigmoid activation function, and you can train it using binary cross-entropy loss. Then get rid of all the bounding boxes for which the objectness score is below some threshold: this will drop all the bounding boxes that don’t actually contain a flower.\n",
    "\n",
    " 2. Find the bounding box with the highest objectness score, and get rid of all the other bounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%). For example, in Figure 14-24, the bounding box with the max objectness score is the thick bounding box over the topmost rose (the objectness score is represented by the thickness of the bounding boxes). The other bounding box over that same rose overlaps a lot with the max bounding box, so we will get rid of it.\n",
    "\n",
    " 3. Repeat step two until there are no more bounding boxes to get rid of.\n",
    " \n",
    "<center><img src=\"img/object-detection-text.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Non-Max Suppression</center></h1>\n",
    "\n",
    "<center><img src=\"img/non-max.jpg\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[10]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>YOLO - You Only Look Once</center></h1>\n",
    "\n",
    "- YOLO algorithm, which was developed in University of Washington, is the state-of-the-art object detection algorithm that uses several components including: CNNs, IoU, and Non-Max Suppression among others: https://pjreddie.com/darknet/yolo/\n",
    "\n",
    "- YOLO is an extremely fast and accurate object detection architecture proposed by Joseph Redmon et al. in a 2015 paper, and subsequently improved in 2016 (YOLOv2), in 2018 (YOLOv3), and in 2020 (YOLOv4 & YOLOv5): https://docs.ultralytics.com/\n",
    "\n",
    "- YOLO is so fast that it can run in real time on a video.\n",
    "\n",
    "- YOLOv3’s architecture is quite similar to v1 and v2, but with a few important differences.\n",
    "\n",
    "- It outputs five bounding boxes for each grid cell (instead of just one), and each bounding box comes with an objectness score. It also outputs 20 class probabilities per grid cell, as it was trained on the PASCAL VOC dataset, which contains 20 classes. That’s a total of 45 numbers per grid cell: 5 bounding boxes, each with 4 coordinates, plus 5 objectness scores, plus 20 class probabilities.\n",
    "\n",
    "- Instead of predicting the absolute coordinates of the bounding box centers, YOLOv3 predicts an offset relative to the coordinates of the grid cell, where (0, 0) means the top left of that cell and (1, 1) means the bottom right. For each grid cell, YOLOv3 is trained to predict only bounding boxes whose center lies in that cell (but the bounding box itself generally extends well beyond the grid cell). YOLOv3 applies the logistic activation function to the bounding box coordinates to ensure they remain in the 0 to 1 range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>YOLOv3 Cont.</center></h1>\n",
    "\n",
    "- Before training the neural net, YOLOv3 finds five representative bounding box dimensions, called **anchor boxes** (or bounding box priors). It does this by applying the K-Means algorithm to the height and width of the training set bounding boxes. For example, if the training images contain many pedestrians, then one of the anchor boxes will likely have the dimensions of a typical pedestrian. Then when the neural net predicts five bounding boxes per grid cell, it actually predicts how much to rescale each of the anchor boxes.\n",
    "\n",
    "- For example, suppose one anchor box is 100 pixels tall and 50 pixels wide, and the network predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for one of the grid cells). This will result in a predicted bounding box of size 150 × 45 pixels. To be more precise, for each grid cell and each anchor box, the network predicts the log of the vertical and horizontal rescaling factors. Having these priors makes the network more likely to predict bounding boxes of the appropriate dimensions, and it also speeds up training because it will more quickly learn what reasonable bounding boxes look like.\n",
    "\n",
    "- The network is trained using images of different scales: every few batches during training, the network randomly chooses a new image dimension (from 330 × 330 to 608 × 608 pixels). This allows the network to learn to detect objects at different scales. Moreover, it makes it possible to use YOLOv3 at different scales: the smaller scale will be less accurate but faster than the larger scale, so you can choose the right trade-off for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>YOLOv5</center></h1>\n",
    "\n",
    "- Several YOLO implementations built using TensorFlow are available on GitHub. In particular, check out Zihao Zang’s TensorFlow 2 implementation of YOLOv3: https://github.com/zzh8829/yolov3-tf2\n",
    "\n",
    "- The latest YOLO version is YOLOv5 which is implemented using PyTorch.\n",
    "\n",
    "- \"*YOLOv5 is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development*\". \n",
    "\n",
    "    - GitHub: https://github.com/ultralytics/yolov5\n",
    "\n",
    "    - Docs: https://docs.ultralytics.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Mean Average Precision (mAP)</center></h1>\n",
    "\n",
    "- A very common metric used in object detection tasks is the **mean Average Precision (mAP)**. “Mean Average” sounds a bit redundant, doesn’t it? To understand this metric, let’s go back to two classification metrics we discussed in Chapter 3: precision and recall.\n",
    "\n",
    "- Remember the trade-off: the higher the recall, the lower the precision. You can visualize this in a precision/recall curve (see Figure 3-5). To summarize this curve into a single number, we could compute its area under the curve (AUC). But note that the precision/recall curve may contain a few sections where precision actually goes up when recall increases, especially at low recall values. This is one of the motivations for the mAP metric.\n",
    "\n",
    "- Suppose the classifier has 90% precision at 10% recall, but 96% precision at 20% recall. There’s really no trade-off here: it simply makes more sense to use the classifier at 20% recall rather than at 10% recall, as you will get both higher recall and higher precision. So instead of looking at the precision at 10% recall, we should really be looking at the maximum precision that the classifier can offer with at least 10% recall. It would be 96%, not 90%.\n",
    "\n",
    "- Therefore, one way to get a fair idea of the model’s performance is to compute the maximum precision you can get with at least 0% recall, then 10% recall, 20%, and so on up to 100%, and then calculate the mean of these maximum precisions. This is called the Average Precision (AP) metric. Now when there are more than two classes, we can compute the AP for each class, and then compute the mean AP (mAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>mAP in Object Detection</center></h1>\n",
    "\n",
    "- In an object detection system, there is an additional level of complexity: what if the system detected the correct class, but at the wrong location (i.e., the bounding box is completely off)? Surely we should not count this as a positive prediction.\n",
    "\n",
    "- One approach is to define an IOU threshold: for example, we may consider that a prediction is correct only if the IOU is greater than, say, 0.5, and the predicted class is correct. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%, or sometimes just $AP_{50}$). In some competitions (such as the PASCAL VOC challenge), this is what is done. In others (such as the COCO competition), the mAP is computed for different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the mean of all these mAPs (noted AP@[.50:.95] or AP@[.50:0.05:.95]). That’s a mean mean average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Face Recognition and Face Landmarking</center></h1>\n",
    "\n",
    "- **Face Landmark Detection** (or Face Landmarking) is a computer vision task to detect and track certain keypoints from a human face. Face Landmarking plays an important role in applications such as biometric recognition and identification as well as understanding mental states from human face.\n",
    "\n",
    "- For instance, the keypoints can be used to detect a human’s head pose position and rotation. With that, we can track whether a driver is paying attention or not. As another example, we can use the keypoints for applying an augmented reality easier.\n",
    "\n",
    "- Multiple NN-based approaches have been proposed for landmark detection tasks, particularly CNN-based approaches. Here is one example: https://github.com/yinguobing/cnn-facial-landmark\n",
    "\n",
    "<center><img src=\"img/face-recog.jpg\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[12]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Face Recognition with One-Shot Learning</center></h1>\n",
    "\n",
    "<center><img src=\"img/one-shot.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[9]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Triplet Loss</center></h1>\n",
    "\n",
    "<center><img src=\"img/triplet_loss.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Triplet Loss</center></h1>\n",
    "\n",
    "<center><img src=\"img/triplet_loss-2.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Facial Expression Recognition</center></h1>\n",
    "\n",
    "**Demo:**\n",
    "\n",
    "https://github.com/justadudewhohacks/face-api.js#age-estimation--gender-recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Semantic Segmentation</center></h1>\n",
    "\n",
    "- In **semantic segmentation**, each pixel is classified according to the class of the object it belongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26. Note that different objects of the same class are not distinguished. For example, all the bicycles on the right side of the segmented image end up as one big lump of pixels.\n",
    "\n",
    "- The main difficulty in this task is that when images go through a regular CNN, they gradually lose their spatial resolution (due to the layers with strides greater than 1); so, a regular CNN may end up knowing that there’s a person somewhere in the bottom left of the image, but it will not be much more precise than that.\n",
    "\n",
    "- Just like for object detection, there are many different approaches to tackle this problem, some quite complex. However, a fairly simple solution was proposed in the 2015 paper by Jonathan Long et al. The authors start by taking a pre‐trained CNN and turning it into a special architecture called **Fully Convolutional Networks (FCN)**. The authors pointed out that you could replace the dense layers at the top of a CNN by convolutional layers. The CNN applies an overall stride of 32 to the input image (i.e., if you add up all the strides greater than 1), meaning the last layer outputs feature maps that are 32 times smaller than the input image. This is clearly too coarse, so they add a single **upsampling layer** that multiplies the resolution by 32.\n",
    "\n",
    "<center><img src=\"img/fig-14-26.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Future Work</center></h1>\n",
    "\n",
    "- As you can see, the field of Deep Computer Vision is vast and moving fast, with all sorts of architectures popping out every year, all based on CNNs.\n",
    "\n",
    "- The progress made in just a few years has been astounding, and researchers are now focusing on harder and harder problems, such as **adversarial learning** (which attempts to make the network more resistant to images designed to fool it), explainability (understanding why the network makes a specific classification), realistic image generation , and single-shot learning (a system that can recognize an object after it has seen it just once).\n",
    "\n",
    "- Some even explore completely novel architectures, such as Geoffrey Hinton’s **Capsule Networks**, or **Attention**-based architectures and **Transofmrers** which are used in language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>References</center></h1>\n",
    "\n",
    "[1] Hands-On ML Textbook Edition-2 2019\n",
    "\n",
    "[2] Deep Learning [Textbook](http://www.deeplearningbook.org/contents/convnets.html) by Ian Goodfellow et al.\n",
    "\n",
    "[3] Andrew Ng's CNN Course in [Coursera](https://www.coursera.org/learn/convolutional-neural-networks?=)\n",
    "\n",
    "[4] https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e\n",
    "\n",
    "[5] https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee\n",
    "\n",
    "[6] https://github.com/aleju/imgaug\n",
    "\n",
    "[7] https://towardsdatascience.com/object-detection-with-10-lines-of-code-d6cb4d86f606\n",
    "\n",
    "[8] https://github.com/justadudewhohacks/face-api.js#age-estimation--gender-recognition\n",
    "\n",
    "[9] https://blog.netcetera.com/face-recognition-using-one-shot-learning-a7cf2b91e96c\n",
    "\n",
    "[10] https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/\n",
    "\n",
    "[11] https://pjreddie.com/darknet/yolo/\n",
    "\n",
    "[12] https://www.nec.com/en/global/solutions/biometrics/face/index.html\n",
    "\n",
    "[13] https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n",
    "\n",
    "[14] https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/\n",
    "\n",
    "[15] https://www.mathworks.com/videos/introduction-to-deep-learning-what-are-convolutional-neural-networks--1489512765771.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
